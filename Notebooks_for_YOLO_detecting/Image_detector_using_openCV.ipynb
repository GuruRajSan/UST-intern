{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was written on colab, it doesn't require CUDA or CUDNN.\n",
    "This notebook is used to perform object detection using trained YOLO models. It uses openCV to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important  file and folder locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to folder containing all test-images\n",
    "Image_folder_path = '/content/drive/MyDrive/Computer_Vision/Yolo_builds/Helmet_detection/Test_images'\n",
    "\n",
    "# Pre-trained Yolo weights and config file\n",
    "Weight_file = '/content/drive/MyDrive/Computer_Vision/Yolo_builds/Helmet_detection/Weights_save/yolov3-voc_4500.weights' #Path to weight file\n",
    "Conf_file = '/content/drive/MyDrive/Computer_Vision/Yolo_builds/Helmet_detection/yolov3-voc.cfg' #Path to conf file\n",
    "\n",
    "# Path of TXT file containing names of classes on which the model was trained\n",
    "Class_file = '/content/drive/MyDrive/Computer_Vision/Yolo_builds/Helmet_detection/class_names.txt'\n",
    "\n",
    "# Directory where the detected images must be put\n",
    "Output_dir = '/content/drive/MyDrive/Computer_Vision/Yolo_builds/Helmet_detection/Detected_images/YoloV3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQP3MzWkjCRu"
   },
   "source": [
    "## Installations and Importations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "otZw7JwH-bT8"
   },
   "outputs": [],
   "source": [
    "#!pip install GitPython\n",
    "#Repo.clone_from(\"https://github.com/pjreddie/darknet\", \"/sample_data/darknet\")\n",
    "\n",
    "import cv2 as cv \n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from google.colab.patches import cv2_imshow  \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wPP1KvdkMT4y",
    "outputId": "6d789520-5d06-4ed8-a567-82b671496173"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# MOUNT DRIVE - (Mounted drive can be accessed at /content/drive/MyDrive)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSnaDeQmkeZj"
   },
   "source": [
    "## Get the image files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t499yP8ei-cK",
    "outputId": "850cc5d4-e054-4718-f192-77666164c24f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of images =  100\n",
      "['hard_hat_workers900.png', 'hard_hat_workers903.png', 'hard_hat_workers904.png', 'hard_hat_workers902.png', 'hard_hat_workers901.png', 'hard_hat_workers947.png', 'hard_hat_workers934.png', 'hard_hat_workers939.png', 'hard_hat_workers949.png', 'hard_hat_workers913.png', 'hard_hat_workers909.png', 'hard_hat_workers917.png', 'hard_hat_workers950.png', 'hard_hat_workers952.png', 'hard_hat_workers942.png', 'hard_hat_workers915.png', 'hard_hat_workers971.png', 'hard_hat_workers922.png', 'hard_hat_workers919.png', 'hard_hat_workers930.png', 'hard_hat_workers912.png', 'hard_hat_workers967.png', 'hard_hat_workers910.png', 'hard_hat_workers931.png', 'hard_hat_workers935.png', 'hard_hat_workers927.png', 'hard_hat_workers961.png', 'hard_hat_workers914.png', 'hard_hat_workers944.png', 'hard_hat_workers962.png', 'hard_hat_workers956.png', 'hard_hat_workers943.png', 'hard_hat_workers965.png', 'hard_hat_workers921.png', 'hard_hat_workers957.png', 'hard_hat_workers918.png', 'hard_hat_workers929.png', 'hard_hat_workers960.png', 'hard_hat_workers970.png', 'hard_hat_workers905.png', 'hard_hat_workers953.png', 'hard_hat_workers911.png', 'hard_hat_workers916.png', 'hard_hat_workers906.png', 'hard_hat_workers926.png', 'hard_hat_workers946.png', 'hard_hat_workers968.png', 'hard_hat_workers959.png', 'hard_hat_workers963.png', 'hard_hat_workers936.png', 'hard_hat_workers937.png', 'hard_hat_workers924.png', 'hard_hat_workers948.png', 'hard_hat_workers928.png', 'hard_hat_workers925.png', 'hard_hat_workers941.png', 'hard_hat_workers966.png', 'hard_hat_workers933.png', 'hard_hat_workers938.png', 'hard_hat_workers920.png', 'hard_hat_workers945.png', 'hard_hat_workers954.png', 'hard_hat_workers969.png', 'hard_hat_workers923.png', 'hard_hat_workers955.png', 'hard_hat_workers940.png', 'hard_hat_workers907.png', 'hard_hat_workers951.png', 'hard_hat_workers964.png', 'hard_hat_workers908.png', 'hard_hat_workers958.png', 'hard_hat_workers932.png', 'hard_hat_workers990.png', 'hard_hat_workers972.png', 'hard_hat_workers984.png', 'hard_hat_workers976.png', 'hard_hat_workers974.png', 'hard_hat_workers995.png', 'hard_hat_workers979.png', 'hard_hat_workers988.png', 'hard_hat_workers992.png', 'hard_hat_workers994.png', 'hard_hat_workers999.png', 'hard_hat_workers986.png', 'hard_hat_workers973.png', 'hard_hat_workers982.png', 'hard_hat_workers981.png', 'hard_hat_workers989.png', 'hard_hat_workers997.png', 'hard_hat_workers977.png', 'hard_hat_workers996.png', 'hard_hat_workers980.png', 'hard_hat_workers983.png', 'hard_hat_workers998.png', 'hard_hat_workers991.png', 'hard_hat_workers993.png', 'hard_hat_workers985.png', 'hard_hat_workers978.png', 'hard_hat_workers975.png', 'hard_hat_workers987.png']\n"
     ]
    }
   ],
   "source": [
    "# Get all images files names from the image folder\n",
    "IMAGE_FILES = os.listdir(Image_folder_path)\n",
    "print('No of images = ',len(IMAGE_FILES))\n",
    "print(IMAGE_FILES)\n",
    "\n",
    "\n",
    "# The image parameters to be considered for all images in the observed data\n",
    "Width = 416\n",
    "Height = 416\n",
    "scale = 1/255\n",
    "\n",
    "# Store all images into a list(also display them)\n",
    "IMAGES = []\n",
    "for filename in IMAGE_FILES:\n",
    "    image = cv.imread(Image_folder_path + '/' + filename)\n",
    "    IMAGES.append(cv.resize(image, (Width, Height)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDi_1DXMk8oZ"
   },
   "source": [
    "# The model is loaded here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5KyPCbrDGC_M",
    "outputId": "e41cda64-13ff-43fd-c66d-c3be63ebcaca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['person', 'head', 'helmet']\n",
      "0.5716800689697266\n"
     ]
    }
   ],
   "source": [
    "# read class names from text file\n",
    "classes = None\n",
    "with open(Class_file, 'r') as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "print(classes)\n",
    "\n",
    "# generate different colors for different classes \n",
    "COLORS = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    "\n",
    "net = cv.dnn.readNet(Conf_file, Weight_file)\n",
    "\n",
    "# create input blob \n",
    "blob = cv.dnn.blobFromImages(IMAGES, scale, (Width,Height), (0,0,0), True, crop=False)\n",
    "\n",
    "# set input blob for the network\n",
    "net.setInput(blob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some useful functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JNqJdanRHlwo"
   },
   "outputs": [],
   "source": [
    "# function to get the output layer names in the architecture\n",
    "def get_output_layers(net):\n",
    "    layer_names = net.getLayerNames()\n",
    "    output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "    return output_layers\n",
    "\n",
    "# function to draw bounding box on the detected object with class name\n",
    "def draw_bounding_box(img, class_id, confidence, x, y, x_plus_w, y_plus_h):\n",
    "    label = str(classes[class_id])+', '+str(round(confidence, 3))\n",
    "    color = COLORS[class_id]\n",
    "    cv.rectangle(img, (x,y), (x_plus_w,y_plus_h), color, 2)\n",
    "    #cv.putText(img, label, (x-10,y-10), cv.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The detection happens here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "8ouZs77NSkag",
    "outputId": "0f79085c-4f93-49e0-bacd-0fbb3776832e"
   },
   "outputs": [],
   "source": [
    "## # Run inference through the network and gather predictions from output layers\n",
    "start = time.time()\n",
    "outs = net.forward(get_output_layers(net))\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "\n",
    "# Bounding box prediction Thresholds\n",
    "conf_threshold = 0.3\n",
    "nms_threshold = 0.3\n",
    "\n",
    "# For each image output\n",
    "for output_no in range(len(IMAGES)):\n",
    "    image = np.copy(IMAGES[output_no]) #Get the input image to be detected on\n",
    "    class_ids = [] #Stores the passing bounding box class-id\n",
    "    confidences = [] #Stores the passing bounding box confidence\n",
    "    boxes = [] #Stores the passing bounding box [x, y, w, h]\n",
    "\n",
    "    # For each Scale output in each image output\n",
    "    for Scale_prediction in outs:\n",
    "\n",
    "        # No of detection box predictions = (number of anchor boxes per scale)*(width/32)*(height/32)*(scale number)^2\n",
    "        # For each detection box prediction take up or ignore the bounding box based on confidence value\n",
    "        for detection in Scale_prediction[output_no]:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > conf_threshold:          \n",
    "                center_x = int(detection[0] * Width)\n",
    "                center_y = int(detection[1] * Height)\n",
    "                w = int(detection[2] * Width)\n",
    "                h = int(detection[3] * Height)\n",
    "                x = center_x - w / 2\n",
    "                y = center_y - h / 2\n",
    "                class_ids.append(class_id)\n",
    "                confidences.append(float(confidence))\n",
    "                boxes.append([x, y, w, h])\n",
    "    \n",
    "    # Apply non-max suppression\n",
    "    indices = cv.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)\n",
    "\n",
    "    # Go through the detections remaining after nms and draw bounding box\n",
    "    for i in indices:\n",
    "        i = i[0]\n",
    "        box = boxes[i]\n",
    "        x = box[0]\n",
    "        y = box[1]\n",
    "        w = box[2]\n",
    "        h = box[3]\n",
    "        draw_bounding_box(image, class_ids[i], confidences[i], round(x), round(y), round(x+w), round(y+h))\n",
    "\n",
    "    # display output image \n",
    "    cv2_imshow(image) #Image must be given in BGR format to cv's imshow\n",
    "    #cv.imwrite(Output_dir+IMAGE_FILES[output_no], image)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Image_detector_using_openCV.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
